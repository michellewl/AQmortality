{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66bc4f8-202c-49a8-af9c-a17f4948a6a7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba59ed30-a2a0-4de8-bcbe-be2b42385c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/mwlw3/miniconda3/envs/AQmort/lib/python3.8/site-packages/pyproj/__init__.py:89: UserWarning: pyproj unable to set database path.\n",
      "  _pyproj_global_context_initialize()\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from os import makedirs, path, listdir, remove\n",
    "import httplib2\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import zipfile as zpf\n",
    "import geopandas as gpd\n",
    "from shutil import rmtree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b2fce-2f24-4481-bd52-1636d4865986",
   "metadata": {},
   "source": [
    "# Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e245913f-8fbd-4af8-b9da-fc649eb976dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"NO2\"\n",
    "region = \"London\"\n",
    "start_date = \"1996-01-01\"\n",
    "end_date = \"2021-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9dc51b3-3ae1-4da8-99c3-5b42ed5e8972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data path\n",
    "folder = path.abspath(\"tmp\")\n",
    "if not path.exists(folder):\n",
    "    makedirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5266dddf-f47c-4ed2-9003-64d7bd63998c",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb704c83-d8be-4d0a-893e-722191a799c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "# Get LAQN site codes\n",
    "url_sites = f\"http://api.erg.kcl.ac.uk/AirQuality/Information/MonitoringSites/GroupName={region}/Json\"\n",
    "               \n",
    "london_sites = requests.get(url_sites)\n",
    "sites_df = pd.DataFrame(london_sites.json()['Sites']['Site'])\n",
    "site_codes = sites_df[\"@SiteCode\"].tolist()\n",
    "print(len(site_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00817cf1-dce4-41e6-9462-711f95971bed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working on site BT8:  10%|â–ˆ         | 26/250 [03:53<21:15,  5.69s/it]"
     ]
    }
   ],
   "source": [
    "# Download LAQN data\n",
    "\n",
    "laqn_df = pd.DataFrame()\n",
    "\n",
    "progress_bar = tqdm(site_codes)\n",
    "\n",
    "for site_code in progress_bar:\n",
    "    progress_bar.set_description(f'Working on site {site_code}')\n",
    "    url_species = f\"http://api.erg.kcl.ac.uk/AirQuality/Data/SiteSpecies/SiteCode={site_code}/SpeciesCode={species}/StartDate={start_date}/EndDate={end_date}/csv\"\n",
    "    cur_df = pd.read_csv(url_species)\n",
    "    cur_df.columns = [\"date\", site_code]\n",
    "    cur_df.set_index(\"date\", drop=True, inplace=True)\n",
    "\n",
    "#     try:\n",
    "    if laqn_df.empty:\n",
    "        laqn_df = cur_df.copy()\n",
    "    else:\n",
    "        laqn_df = laqn_df.join(cur_df.copy(), how=\"outer\")\n",
    "\n",
    "#     except ValueError:  # Trying to join with duplicate column names\n",
    "#         rename_dict = {}\n",
    "#         for x in list(set(cur_df.columns).intersection(laqn_df.columns)):\n",
    "#             rename_dict.update({x: f\"{x}_\"})\n",
    "#             print(f\"Renamed duplicated column:\\n{rename_dict}\")\n",
    "#         laqn_df.rename(mapper=rename_dict, axis=\"columns\", inplace=True)\n",
    "#         if laqn_df.empty:\n",
    "#             laqn_df = cur_df.copy()\n",
    "#         else:\n",
    "#             laqn_df = laqn_df.join(cur_df.copy(), how=\"outer\")\n",
    "#         if verbose:\n",
    "#             print(f\"Joined.\")\n",
    "\n",
    "#     except KeyError:  # Trying to join along indexes that don't match\n",
    "#         print(f\"Troubleshooting {site_code}...\")\n",
    "#         cur_df.index = cur_df.index + \":00\"\n",
    "#         if laqn_df.empty:\n",
    "#             laqn_df = cur_df.copy()\n",
    "#         else:\n",
    "#             laqn_df = laqn_df.join(cur_df.copy(), how=\"outer\")\n",
    "#         print(f\"{site_code} joined.\")\n",
    "\n",
    "#     print(\"Data download complete. Removing sites with 0 data...\")\n",
    "laqn_df.dropna(axis=\"columns\", how=\"all\", inplace=True)\n",
    "laqn_df.to_csv(path.join(folder, f\"LAQN_{species}_{start_date}_{end_date}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394a23f-97ca-40fd-9f24-55c2c09f3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download LAQN metadata\n",
    "request = requests.get(\"http://api.erg.kcl.ac.uk/AirQuality/Information/MonitoringSites/GroupName=London/Json\")\n",
    "london_sites_df = pd.DataFrame(request.json()['Sites']['Site'])\n",
    "london_sites_df = london_sites_df.loc[london_sites_df[\"@Latitude\"]!=\"\"]\n",
    "london_sites_df = london_sites_df.loc[london_sites_df[\"@Latitude\"].astype(float)>51]\n",
    "\n",
    "lat = london_sites_df[\"@Latitude\"].values\n",
    "lon = london_sites_df[\"@Longitude\"].values\n",
    "\n",
    "# Create a geopandas dataframe\n",
    "london_sites_gdf = gpd.GeoDataFrame(london_sites_df, geometry=gpd.points_from_xy(lon, lat))\n",
    "london_sites_gdf.to_file(path.join(folder, \"LAQN_sites.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5a4a4-1387-475b-833d-d6768609728b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download shape files for map plotting\n",
    "\n",
    "# Get the web links for the borough & ward coordinate data files\n",
    "web_address = \"https://data.london.gov.uk/dataset/i-trees-canopy-ward-data\"\n",
    "status, response = httplib2.Http().request(web_address)\n",
    "link_dict = {}\n",
    "\n",
    "for link in BeautifulSoup(response, parse_only=SoupStrainer('a')):\n",
    "    if link.has_attr('href') and link[\"href\"].split(\".\")[-1]==\"zip\":\n",
    "        link_dict[link['href'].split(\"/\")[-1].split(\".\")[0]] = f\"https://data.london.gov.uk/{link['href']}\"\n",
    "print(f\"{len(link_dict)} links found\")\n",
    "        \n",
    "# Download the borough & ward coordinate data files and unzip them\n",
    "if not path.exists(path.join(folder, \"i-Trees\")):\n",
    "    makedirs(path.join(folder, \"i-Trees\"))\n",
    "print(\"Downloading shape data...\")\n",
    "for url in tqdm(link_dict.values()):\n",
    "    request = requests.get(url)\n",
    "    filepath = path.join(folder, \"i-Trees\", path.basename(url))\n",
    "    file = open(filepath, 'wb')\n",
    "    file.write(request.content)\n",
    "    file.close()\n",
    "    zpf.ZipFile(filepath, 'r').extractall(path.join(folder, \"i-Trees\"))\n",
    "    \n",
    "# Compile a geopandas dataframe of the london wards coordinates\n",
    "print(\"Compiling geopandas dataframe...\")\n",
    "london_wards_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "for borough in tqdm(link_dict.keys()):\n",
    "    borough_folder = path.join(folder, \"i-Trees\", borough)\n",
    "    shapefiles = [file for file in listdir(borough_folder) if file.split(\".\")[-1]==\"shp\"]\n",
    "    for shapefile in shapefiles:\n",
    "        gdf = gpd.read_file(path.join(borough_folder, shapefile))\n",
    "        if london_wards_gdf.empty:\n",
    "            london_wards_gdf = gdf\n",
    "        else:\n",
    "            london_wards_gdf = pd.concat([london_wards_gdf, gdf])\n",
    "\n",
    "# Convert from ward level to boroughs\n",
    "london_boroughs_gdf = london_wards_gdf.dissolve(by=\"BOROUGH\")\n",
    "london_boroughs_gdf.to_file(path.join(folder, \"london_boroughs_coordinates.shp\"))\n",
    "rmtree(path.join(folder, \"i-Trees\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c1f409-fc25-4e72-9775-c01c01696092",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d87bfb-5d95-4b21-be60-01fecde973e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LAQN data\n",
    "laqn_df = pd.read_csv(path.join(folder, f\"LAQN_{species}_{start_date}_{end_date}.csv\"))\n",
    "print(laqn_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc8dfe-30ca-4d89-af6b-0c0a784848b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load map file\n",
    "london_boroughs_gdf = gpd.read_file(path.join(folder, \"london_boroughs_coordinates.shp\"))\n",
    "print(london_boroughs_gdf.shape)\n",
    "london_boroughs_gdf.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c6671e-aaa2-4ab3-9f57-966960f3de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LAQN metadata\n",
    "london_sites_gdf = gpd.read_file(path.join(folder, \"LAQN_sites.shp\"))\n",
    "print(london_sites_gdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca338f6-a5e8-4803-a050-53941e92dbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AQmort",
   "language": "python",
   "name": "aqmort"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
