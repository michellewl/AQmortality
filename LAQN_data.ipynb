{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conda environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When accessing this notebook via the JASMIN Jupyter Notebook service, select the correct conda environment from the list of available kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from os import makedirs, path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "LAQN class for downloading &  processing of LAQN data. This code is adapted from my [COVID-19 repo](https://github.com/michellewl/COVID-19/blob/master/data/LAQN_class.py) (note that the borough averaging code has not been included here). See [here](https://github.com/michellewl/NO2-breast-cancer/tree/master/data/LAQN/download) for details on the source code (originally written for MRes project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAQNData():\n",
    "    def __init__(self, url, home_folder, species, start_date, end_date):\n",
    "        self.url = url\n",
    "        self.home_folder = home_folder\n",
    "        self.species = species\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        \n",
    "        if not path.exists(self.home_folder):\n",
    "            makedirs(self.home_folder)\n",
    "        \n",
    "        london_sites = requests.get(self.url)\n",
    "        self.sites_df = pd.DataFrame(london_sites.json()['Sites']['Site'])\n",
    "        self.site_codes = self.sites_df[\"@SiteCode\"].tolist()\n",
    "\n",
    "    def download(self, verbose=True):\n",
    "        laqn_df = pd.DataFrame()\n",
    "        \n",
    "        if verbose:\n",
    "            progress_bar = tqdm(self.site_codes)\n",
    "        else:\n",
    "            progress_bar = self.site_codes\n",
    "            \n",
    "        for site_code in progress_bar:\n",
    "            if verbose:\n",
    "                progress_bar.set_description(f'Working on site {site_code}')\n",
    "            url_species = f\"http://api.erg.kcl.ac.uk/AirQuality/Data/SiteSpecies/SiteCode={site_code}/SpeciesCode={self.species}/StartDate={self.start_date}/EndDate={self.end_date}/csv\"\n",
    "            cur_df = pd.read_csv(url_species)\n",
    "            cur_df.columns = [\"date\", site_code]\n",
    "            cur_df.set_index(\"date\", drop=True, inplace=True)\n",
    "\n",
    "            try:\n",
    "                if laqn_df.empty:\n",
    "                    laqn_df = cur_df.copy()\n",
    "                else:\n",
    "                    laqn_df = laqn_df.join(cur_df.copy(), how=\"outer\")\n",
    "\n",
    "            except ValueError:  # Trying to join with duplicate column names\n",
    "                rename_dict = {}\n",
    "                for x in list(set(cur_df.columns).intersection(laqn_df.columns)):\n",
    "                    rename_dict.update({x: f\"{x}_\"})\n",
    "                    print(f\"Renamed duplicated column:\\n{rename_dict}\")\n",
    "                laqn_df.rename(mapper=rename_dict, axis=\"columns\", inplace=True)\n",
    "                if laqn_df.empty:\n",
    "                    laqn_df = cur_df.copy()\n",
    "                else:\n",
    "                    laqn_df = laqn_df.join(cur_df.copy(), how=\"outer\")\n",
    "                if verbose:\n",
    "                    print(f\"Joined.\")\n",
    "\n",
    "            except KeyError:  # Trying to join along indexes that don't match\n",
    "                print(f\"Troubleshooting {site_code}...\")\n",
    "                cur_df.index = cur_df.index + \":00\"\n",
    "                if laqn_df.empty:\n",
    "                    laqn_df = cur_df.copy()\n",
    "                else:\n",
    "                    laqn_df = laqn_df.join(cur_df.copy(), how=\"outer\")\n",
    "                print(f\"{site_code} joined.\")\n",
    "\n",
    "        print(\"Data download complete. Removing sites with 0 data...\")\n",
    "        laqn_df.dropna(axis=\"columns\", how=\"all\", inplace=True)\n",
    "        laqn_df.to_csv(path.join(self.home_folder, f\"{self.species}_hourly_{self.start_date}_{self.end_date}.csv\"))\n",
    "        print(\"Data saved.\")\n",
    "\n",
    "    def resample_time(self, df, key, quantile_step):\n",
    "        df.set_index(\"date\", drop=True, inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        if key == \"D\":\n",
    "            keyword = \"daily\"\n",
    "        if key == \"W\":\n",
    "            keyword = \"weekly\"\n",
    "\n",
    "        save_folder = path.join(self.home_folder, keyword)\n",
    "        if not path.exists(save_folder):\n",
    "            makedirs(save_folder)\n",
    "\n",
    "        aggregation = np.round(np.arange(0, 1 + quantile_step, quantile_step), 2).tolist()\n",
    "\n",
    "        for method in aggregation:\n",
    "            aggregated_df = df.copy().resample(key).quantile(method)\n",
    "            method = f\"{int(method * 100)}_quantile\"\n",
    "            aggregated_df.to_csv(path.join(save_folder, f\"{self.species}_{keyword}_{method}.csv\"), index=True)\n",
    "            print(aggregated_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_folder = path.join(path.abspath(\"\"), \"LAQN_data\")\n",
    "species = \"NO2\"\n",
    "url = \"http://api.erg.kcl.ac.uk/AirQuality/Information/MonitoringSites/GroupName=London/Json\"\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2021-01-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO2_hourly = LAQNData(url, home_folder, species, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "print(len(NO2_hourly.site_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Working on site BN1:   3%|â–Ž         | 6/236 [00:53<32:33,  8.49s/it]"
     ]
    }
   ],
   "source": [
    "NO2_hourly.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AQmort",
   "language": "python",
   "name": "aqmort"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
