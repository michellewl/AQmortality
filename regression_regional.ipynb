{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_classes import LAQNData, HealthData, MetData, IncomeData\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports for classes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from os import path, listdir, environ\n",
    "import wandb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"architecture\": \"MLP_regressor\",\n",
    "    \"train_size\": 0.7,\n",
    "    \"val_size\": 0.15, # Set to False if using linear regressor\n",
    "    \"species\": \"NO2\",\n",
    "    \"spatial_resolution\": \"regional\",\n",
    "    \"temporal_resolution\": \"daily\",\n",
    "    \"input_artifacts\": [\"laqn-regional\", \"met-resample\", \"income-regional\"],\n",
    "    \"met_variables\": [\"temperature\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be moved to model_classes script when finished developing.\n",
    "\n",
    "class MLPRegression():\n",
    "    def __init__(self):\n",
    "        # Initialise the MLPArchitecture class\n",
    "        # Set random seed for torch\n",
    "        \n",
    "    def fit(self, x_train, y_train):\n",
    "        # Training code that loops through epochs\n",
    "        return  regressor\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Produce outputs y\n",
    "        return y\n",
    "    \n",
    "class MLPArchitecture(nn.Module):\n",
    "    def __init__(self, hl_sizes, out_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hl_sizes)-1):\n",
    "            self.hidden_layers.append(nn.Linear(hl_sizes[i], hl_sizes[i + 1], bias=False))\n",
    "        self.output_layer = nn.Linear(hl_sizes[-1], out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through each layer\n",
    "        x = x.view(x.shape[0], x.shape[1] * x.shape[2])\n",
    "        # Feedforward\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = torch.relu(hidden_layer(x))\n",
    "        return self.output_layer(x)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be moved to model_classes script when finished developing.\n",
    "\n",
    "class HealthModel():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.architecture = config[\"architecture\"]\n",
    "        self.train_size = config[\"train_size\"]\n",
    "        self.val_size = config[\"val_size\"]\n",
    "        self.species = config[\"species\"]\n",
    "        self.spatial_resolution = config[\"spatial_resolution\"]\n",
    "        self.temporal_resolution = config[\"temporal_resolution\"]\n",
    "        self.input_artifacts = config[\"input_artifacts\"]\n",
    "        self.met_variables = config[\"met_variables\"]\n",
    "\n",
    "    def preprocess_and_log(self):\n",
    "        with wandb.init(project=\"AQmortality\", job_type=\"split-normalise-data\") as run:\n",
    "            df = pd.DataFrame()\n",
    "            # use dataset artifacts\n",
    "            for artifact in self.input_artifacts:\n",
    "                data_artifact = run.use_artifact(f\"{artifact}:latest\")\n",
    "                data_folder = data_artifact.download()\n",
    "                if artifact == \"met-resample\":\n",
    "                    for variable in self.met_variables:\n",
    "                        file = f\"{variable}.npz\"\n",
    "                        data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "                        if df.empty:\n",
    "                            df = pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"], columns=[variable])\n",
    "                        else:\n",
    "                            df = df.join(pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"], columns=[variable]))\n",
    "                else:\n",
    "                    file = listdir(data_folder)[0]\n",
    "                    data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "                    if df.empty:\n",
    "                        df = pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"], columns=[file.replace(\".npz\", \"\")])\n",
    "                    else:\n",
    "                        df = df.join(pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"], columns=[file.replace(\".npz\", \"\")]))\n",
    "\n",
    "            target_artifact = run.use_artifact(\"mortality-scaled:latest\")\n",
    "            target_folder = target_artifact.download()\n",
    "            data = np.load(path.join(target_folder, \"deaths.npz\"), allow_pickle=True)\n",
    "            df = df.join(pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"]*100000, columns=[\"deaths\"]))\n",
    "            df = df.dropna(axis=0)\n",
    "\n",
    "            # make new train, validation and test artifacts for regional scale data\n",
    "            if self.val_size:\n",
    "                index = {\"train\": df.index[:int(len(df.index)*self.train_size)],\n",
    "                         \"validat\": df.index[int(len(df.index)*self.train_size):int(len(df.index)*(self.train_size+self.val_size))],\n",
    "                        \"test\": df.index[int(len(df.index)*(self.train_size+self.val_size)):]}\n",
    "                subsets = [\"train\", \"validat\", \"test\"]\n",
    "            else:\n",
    "                index = {\"train\": df.index[:int(len(df.index)*self.train_size)],\n",
    "                        \"test\": df.index[int(len(df.index)*self.train_size):]}\n",
    "                subsets = [\"train\", \"test\"]\n",
    "            \n",
    "            scaler = MinMaxScaler()\n",
    "            x_scaler = scaler.fit(df.loc[index[\"train\"]].drop(\"deaths\", axis=1)) # Fit the scaler only to the training set distribution\n",
    "            for subset in subsets:\n",
    "                x = x_scaler.transform(df.loc[index[subset]].drop(\"deaths\", axis=1))\n",
    "                y = df.loc[index[subset]][\"deaths\"].values\n",
    "                z = df.loc[index[subset]].index\n",
    "                subset_data = wandb.Artifact(\n",
    "                            f\"xy_{subset}\", type=\"dataset\",\n",
    "                            description=f\"Input features (normalised) and targets for {subset}ing set.\",\n",
    "                            metadata={\"input_shape\":x.shape,\n",
    "                                     \"target_shape\":y.shape,\n",
    "                                     \"species\": self.species,\n",
    "                                      \"spatial_resolution\": self.spatial_resolution,\n",
    "                                      \"temporal_resolution\": self.temporal_resolution,\n",
    "                                      \"input_artifacts\": self.input_artifacts,\n",
    "                                      \"met_variables\": self.met_variables})\n",
    "                with subset_data.new_file(subset + \".npz\", mode=\"wb\") as file:\n",
    "                    np.savez(file, x=x, y=y, z=z)\n",
    "                run.log_artifact(subset_data)\n",
    "                \n",
    "    def read_data(self, artifact):\n",
    "        with wandb.init(project=\"AQmortality\", job_type=\"read-data\") as run:\n",
    "            data_artifact = run.use_artifact(f\"{artifact}:latest\")\n",
    "            data_folder = data_artifact.download()\n",
    "            file = artifact.replace(\"xy_\", \"\") + \".npz\"\n",
    "            data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "        return data[\"x\"], data[\"y\"]\n",
    "    \n",
    "    def train_and_log(self):\n",
    "        model_type = self.architecture.replace(\"_\", \"-\")\n",
    "        with wandb.init(project=\"AQmortality\", job_type=\"train-regional-model\", config=self.config) as run:\n",
    "            artifact = \"xy_train\"\n",
    "            data_artifact = run.use_artifact(f\"{artifact}:latest\")\n",
    "            data_folder = data_artifact.download()\n",
    "            file = artifact.replace(\"xy_\", \"\") + \".npz\"\n",
    "            data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "            x_train, y_train = data[\"x\"], data[\"y\"]\n",
    "            if model_type == \"linear-regressor\":\n",
    "                regressor = LinearRegression().fit(x_train, y_train)\n",
    "            elif model_type == \"MLP-regressor\":\n",
    "                regressor = MLPRegression().fit(x_train, y_train)\n",
    "                # log model training metrics\n",
    "                wandb.log({\"r_squared\": r2_score(y_train, regressor.predict(x_train)),\n",
    "                           \"mean_squared_error\": mean_squared_error(y_train, regressor.predict(x_train)),\n",
    "                           \"mean_absolute_percentage_error\": self.mape_score(y_train, regressor.predict(x_train))\n",
    "                          })\n",
    "\n",
    "        # log trained model artifact – include input features description\n",
    "            model = wandb.Artifact(\n",
    "                            f\"{model_type}\", type=\"model\",\n",
    "                            description=f\"{model_type} model.\",\n",
    "                            metadata={\"input_shape\":x_train.shape,\n",
    "                                      \"target_shape\":y_train.shape,\n",
    "                                      \"species\": self.species,\n",
    "                                      \"spatial_resolution\": self.spatial_resolution,\n",
    "                                      \"temporal_resolution\": self.temporal_resolution,\n",
    "                                      \"input_artifacts\": self.input_artifacts,\n",
    "                                      \"met_variables\": self.met_variables\n",
    "                                      })\n",
    "        \n",
    "            with model.new_file(\"model.sav\", mode=\"wb\") as file:\n",
    "                joblib.dump(regressor, file)\n",
    "            run.log_artifact(model)\n",
    "    \n",
    "    \n",
    "    def test_and_log(self):\n",
    "        model_type = self.architecture.replace(\"_\", \"-\")\n",
    "        with wandb.init(project=\"AQmortality\", job_type=\"test-regional-model\", config=self.config) as run:\n",
    "            data_dict = {}\n",
    "            if self.val_size:\n",
    "                subsets = [\"train\", \"validat\", \"test\"]\n",
    "            else:\n",
    "                subsets = [\"train\", \"test\"]\n",
    "            for subset in subsets:\n",
    "                data_artifact = run.use_artifact(f\"xy_{subset}:latest\")\n",
    "                data_folder = data_artifact.download()\n",
    "                file = f\"{subset}.npz\"\n",
    "                data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "                data_dict.update({\"x_\"+subset: data[\"x\"], \"y_\"+subset: data[\"y\"], subset+\"_dates\": data[\"z\"]})\n",
    "            # use trained model artifact\n",
    "            model_artifact = run.use_artifact(f\"{model_type}:latest\")\n",
    "            model_folder = model_artifact.download()\n",
    "            regressor = joblib.load(path.join(model_folder, \"model.sav\"))\n",
    "    \n",
    "        # test linear regression model\n",
    "        # log model test metrics\n",
    "            wandb.log({\"r_squared\": r2_score(data_dict[\"y_test\"], regressor.predict(data_dict[\"x_test\"])),\n",
    "                       \"mean_squared_error\": mean_squared_error(data_dict[\"y_test\"], regressor.predict(data_dict[\"x_test\"])),\n",
    "                       \"mean_absolute_percentage_error\": self.mape_score(data_dict[\"y_test\"], regressor.predict(data_dict[\"x_test\"]))\n",
    "                      })\n",
    "        return regressor, data_dict\n",
    "    \n",
    "    def mape_score(self, targets, predictions):\n",
    "        zero_indices = np.where(targets == 0)\n",
    "        targets_drop_zero = np.delete(targets, zero_indices)\n",
    "        prediction_drop_zero = np.delete(predictions, zero_indices)\n",
    "        mape = np.sum(np.abs(targets_drop_zero - prediction_drop_zero)/targets_drop_zero) * 100/len(targets_drop_zero)\n",
    "        return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HealthModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichellewl\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">deft-donkey-166</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/michellewl/AQmortality\" target=\"_blank\">https://wandb.ai/michellewl/AQmortality</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/michellewl/AQmortality/runs/3el6qsh1\" target=\"_blank\">https://wandb.ai/michellewl/AQmortality/runs/3el6qsh1</a><br/>\n",
       "                Run data is saved locally in <code>/home/users/mwlw3/AQmortality/wandb/run-20210524_155113-3el6qsh1</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 173<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d57dbb33eee44249d8098ecd339a1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.25MB of 0.25MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/users/mwlw3/AQmortality/wandb/run-20210524_155113-3el6qsh1/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/users/mwlw3/AQmortality/wandb/run-20210524_155113-3el6qsh1/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">deft-donkey-166</strong>: <a href=\"https://wandb.ai/michellewl/AQmortality/runs/3el6qsh1\" target=\"_blank\">https://wandb.ai/michellewl/AQmortality/runs/3el6qsh1</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.preprocess_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_and_log(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor, data_dict = model.test_and_log(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "if config[\"val_size\"]:\n",
    "    subsets = [\"train\", \"validat\", \"test\"]\n",
    "else:\n",
    "    subsets = [\"train\", \"test\"]\n",
    "for subset in subsets:\n",
    "    subset_df = pd.DataFrame(index=pd.DatetimeIndex(data_dict[f\"{subset}_dates\"]), data={\"observed\":data_dict[f\"y_{subset}\"], \"predicted\":regressor.predict(data_dict[f\"x_{subset}\"])})\n",
    "    if df.empty:\n",
    "        df = subset_df.copy()\n",
    "    else:\n",
    "        df = df.append(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n",
    "plt.axvline(data_dict[\"train_dates\"].max(), color=\"grey\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"deaths per 100,000\")\n",
    "model_title = config[\"architecture\"].replace(\"_\", \" \")\n",
    "plt.suptitle(\"London mortality predictions by linear regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AQmort",
   "language": "python",
   "name": "aqmort"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
