{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_classes import LAQNData, HealthData, MetData, IncomeData\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports for classes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "from os import path, listdir, environ\n",
    "import wandb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc9e3b8ded0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"architecture\": \"MLP_regressor\",\n",
    "    \"train_size\": 0.7,\n",
    "    \"val_size\": 0.15, # Set MLP configs\n",
    "    \"hidden_layer_sizes\": [3, 10],\n",
    "    \"batch_size\": 30, # to False\n",
    "    \"num_epochs\": 100, # if using\n",
    "    \"learning_rate\": 0.001, # linear regressor\n",
    "    \"species\": \"NO2\",\n",
    "    \"spatial_resolution\": \"regional\",\n",
    "    \"temporal_resolution\": \"daily\",\n",
    "    \"input_artifacts\": [\"laqn-regional\", \"met-resample\", \"income-regional\"],\n",
    "    \"met_variables\": [\"temperature\"]\n",
    "    }\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function. Move to model classes script when finished.\n",
    "\n",
    "def mape_score(targets, predictions):\n",
    "        zero_indices = np.where(targets == 0)\n",
    "        targets_drop_zero = np.delete(targets, zero_indices)\n",
    "        prediction_drop_zero = np.delete(predictions, zero_indices)\n",
    "        mape = np.sum(np.abs(targets_drop_zero - prediction_drop_zero)/targets_drop_zero) * 100/len(targets_drop_zero)\n",
    "        return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be moved to model_classes script when finished developing.\n",
    "\n",
    "class MLPRegression():\n",
    "    def __init__(self, hidden_layer_sizes, output_layer_size=1):\n",
    "        self.hl_sizes = hidden_layer_sizes\n",
    "        self.out_sizes = output_layer_size\n",
    "        # Initialise the MLPArchitecture class\n",
    "        self.model = MLPArchitecture(self.hl_sizes, self.out_sizes)\n",
    "        self.metrics_functions = {\"r2\": r2_score, \"mse\": mean_squared_error, \"mape\": mape_score}\n",
    "        \n",
    "    def fit(self, x_train, y_train, x_val, y_val, batch_size, num_epochs, learning_rate, noise_standard_deviation=False):\n",
    "        # Training code that loops through epochs\n",
    "        model = self.model\n",
    "        \n",
    "        training_dataset = MLPDataset(x_train, y_train, noise_std=noise_standard_deviation)\n",
    "        validation_dataset = MLPDataset(x_val, y_val)\n",
    "\n",
    "        training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "        validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimiser = Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "        \n",
    "        training_loss_history = []\n",
    "        validation_loss_history = []\n",
    "        metrics_scores = {}\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            # Training set\n",
    "            model.train()\n",
    "            loss_sum = 0  # for storing\n",
    "            y_pred_epoch = np.zeros_like(y_train)\n",
    "\n",
    "            for batch_num, data in enumerate(training_dataloader):\n",
    "                inputs_training = data[\"inputs\"]\n",
    "                targets_training = data[\"targets\"]\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "                # Run the forward pass\n",
    "                y_predict = model(inputs_training)\n",
    "                y_pred_epoch[batch_num*batch_size : (batch_num+1)*batch_size] = np.squeeze(y_predict.detach().numpy())\n",
    "                # Compute the loss and gradients\n",
    "                single_loss = criterion(y_predict, targets_training)\n",
    "                single_loss.backward()\n",
    "                # Update the parameters\n",
    "                optimiser.step()\n",
    "\n",
    "                # Calculate loss for storing\n",
    "                loss_sum += single_loss.item()*data[\"targets\"].shape[0]  # Account for different batch size with final batch\n",
    "\n",
    "            training_loss_history.append(loss_sum / len(training_dataset))  # Save the training loss after every epoch\n",
    "            \n",
    "            # Do the same for the validation set\n",
    "            model.eval()\n",
    "            validation_loss_sum = 0\n",
    "            y_pred_val_epoch = np.zeros_like(y_val)\n",
    "            with torch.no_grad():\n",
    "                for batch_num, data in enumerate(validation_dataloader):\n",
    "                    inputs_val = data[\"inputs\"]\n",
    "                    targets_val = data[\"targets\"]\n",
    "                    y_predict_validation = model(inputs_val)\n",
    "                    y_pred_val_epoch[batch_num*batch_size : (batch_num+1)*batch_size] = np.squeeze(y_predict_validation.detach().numpy())\n",
    "                    single_loss = criterion(y_predict_validation, targets_val)\n",
    "                    validation_loss_sum += single_loss.item()*data[\"targets\"].shape[0]\n",
    "                    \n",
    "            for metric in [\"r2\", \"mse\", \"mape\"]:\n",
    "                metrics_scores.update({f\"{metric}_train\": self.metrics_functions[metric](y_train, y_pred_epoch)})\n",
    "                metrics_scores.update({f\"{metric}_val\": self.metrics_functions[metric](y_val, y_pred_val_epoch)})\n",
    "                \n",
    "            # Store the model with smallest validation loss. Check if the validation loss is the lowest BEFORE\n",
    "            # saving it to loss history (otherwise it will not be lower than itself)\n",
    "            if (not validation_loss_history) or validation_loss_sum / len(validation_dataset) < min(validation_loss_history):\n",
    "                self.best_model = deepcopy(model)\n",
    "                best_epoch = epoch\n",
    "                best_metrics = metrics_scores.copy()\n",
    "           \n",
    "            validation_loss_history.append(validation_loss_sum / len(validation_dataset))  # Save the val loss every epoch.\n",
    "\n",
    "\n",
    "            wandb.log({\"training_loss\": loss_sum / len(training_dataset),\n",
    "                      \"validation_loss\": validation_loss_sum / len(validation_dataset), \n",
    "                      \"r2_train\": metrics_scores[\"r2_train\"],\n",
    "                      \"r2_val\": metrics_scores[\"r2_val\"],\n",
    "                      \"mean_squared_error_train\": metrics_scores[\"mse_train\"],\n",
    "                      \"mean_squared_error_val\": metrics_scores[\"mse_val\"],\n",
    "                      \"mean_absolute_percentage_error_train\": metrics_scores[\"mape_train\"],\n",
    "                       \"mean_absolute_percentage_error_val\": metrics_scores[\"mape_val\"]\n",
    "                      },\n",
    "                      step=epoch)\n",
    "            \n",
    "#                 # Save the model every 2 epochs\n",
    "#             if epoch % 2 == 0:\n",
    "#                 torch.save({\n",
    "#                     \"total_epochs\": epoch,\n",
    "#                     \"final_state_dict\": model.state_dict(),\n",
    "#                     \"optimiser_state_dict\": optimiser.state_dict(),\n",
    "#                     \"training_loss_history\": training_loss_history,\n",
    "#                     \"best_state_dict\": best_model.state_dict(),\n",
    "#                     \"best_epoch\": best_epoch,\n",
    "#                     \"validation_loss_history\": validation_loss_history,\n",
    "#                 }, save_path)\n",
    "\n",
    "        \n",
    "        wandb.log({\"best_epoch\": best_epoch, \n",
    "                   \"best_r_squared_train\": best_metrics[\"r2_train\"],\n",
    "                   \"best_r_squared_val\": best_metrics[\"r2_val\"],\n",
    "                   \"best_mean_squared_error_train\": best_metrics[\"mse_train\"],\n",
    "                   \"best_mean_squared_error_val\": best_metrics[\"mse_val\"],\n",
    "                   \"best_mean_absolute_percentage_error_train\": best_metrics[\"mape_train\"],\n",
    "                   \"best_mean_absolute_percentage_error_val\": best_metrics[\"mape_val\"]\n",
    "                  })\n",
    "        \n",
    "        return  self.best_model, best_epoch\n",
    "    \n",
    "    def evaluate(self, checkpoint, x_test, y_test, batch_size, noise_standard_deviation=False):\n",
    "        model = self.model\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        epoch = checkpoint[\"epoch\"]\n",
    "        \n",
    "        test_dataset = MLPDataset(x_test, y_test, noise_std=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        \n",
    "        metrics_scores = {}\n",
    "        \n",
    "        model.eval()\n",
    "        y_predict = np.zeros_like(y_test)\n",
    "        with torch.no_grad():\n",
    "            for batch_num, data in enumerate(test_dataloader):\n",
    "                inputs = data[\"inputs\"]\n",
    "                targets = data[\"targets\"]\n",
    "                outputs = model(inputs)\n",
    "                y_predict[batch_num*batch_size : (batch_num+1)*batch_size] = np.squeeze(outputs.detach().numpy())\n",
    "                \n",
    "        for metric in [\"r2\", \"mse\", \"mape\"]:\n",
    "            metrics_scores.update({f\"{metric}_test\": self.metrics_functions[metric](y_test, y_predict)})\n",
    "    \n",
    "        wandb.log({\"r2_test\": metrics_scores[\"r2_test\"],\n",
    "                   \"mean_squared_error_test\": metrics_scores[\"mse_test\"],\n",
    "                   \"mean_absolute_percentage_error_test\": metrics_scores[\"mape_test\"]})\n",
    "        \n",
    "    \n",
    "    def predict(self, checkpoint, x, y, batch_size):\n",
    "        model = self.model\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        dataset = MLPDataset(x, y, noise_std=False)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "               \n",
    "        model.eval()\n",
    "        y_predict = np.zeros_like(y)\n",
    "        with torch.no_grad():\n",
    "            for batch_num, data in enumerate(dataloader):\n",
    "                inputs = data[\"inputs\"]\n",
    "                targets = data[\"targets\"]\n",
    "                outputs = model(inputs)\n",
    "                y_predict[batch_num*batch_size : (batch_num+1)*batch_size] = np.squeeze(outputs.detach().numpy())\n",
    "        return y_predict\n",
    "        \n",
    "    \n",
    "class MLPArchitecture(nn.Module):\n",
    "    def __init__(self, hl_sizes, out_size):\n",
    "        super(MLPArchitecture, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hl_sizes)-1):\n",
    "            self.hidden_layers.append(nn.Linear(hl_sizes[i], hl_sizes[i + 1], bias=False))\n",
    "        self.output_layer = nn.Linear(hl_sizes[-1], out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through each layer\n",
    "        x = x.view(x.shape[0], x.shape[1])\n",
    "        # Feedforward\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = torch.relu(hidden_layer(x))\n",
    "        return self.output_layer(x)\n",
    "       \n",
    "class MLPDataset(Dataset):\n",
    "    def __init__(self, x, y, noise_std=False):\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()[:, None]\n",
    "        self.noise_std = noise_std  # Standard deviation of Gaussian noise\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.size()[0]\n",
    "\n",
    "    def nfeatures(self):\n",
    "        return self.x.size()[-1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x1 = self.x[index]\n",
    "        y1 = self.y[index]\n",
    "        if self.noise_std:\n",
    "            noise = torch.randn_like(x1)*self.noise_std\n",
    "            x1 = x1 + noise\n",
    "        return {\"inputs\": x1, \"targets\": y1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be moved to model_classes script when finished developing.\n",
    "\n",
    "class HealthModel():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.architecture = config[\"architecture\"]\n",
    "        self.train_size = config[\"train_size\"]\n",
    "        self.val_size = config[\"val_size\"]\n",
    "        self.hidden_layer_sizes = config[\"hidden_layer_sizes\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.num_epochs = config[\"num_epochs\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.species = config[\"species\"]\n",
    "        self.spatial_resolution = config[\"spatial_resolution\"]\n",
    "        self.temporal_resolution = config[\"temporal_resolution\"]\n",
    "        self.input_artifacts = config[\"input_artifacts\"]\n",
    "        self.met_variables = config[\"met_variables\"]\n",
    "\n",
    "    def preprocess_and_log(self):\n",
    "        with wandb.init(project=\"AQmortality\", job_type=\"split-normalise-data\") as run:\n",
    "            df = pd.DataFrame()\n",
    "            # use dataset artifacts\n",
    "            for artifact in self.input_artifacts:\n",
    "                data_artifact = run.use_artifact(f\"{artifact}:latest\")\n",
    "                data_folder = data_artifact.download()\n",
    "                if artifact == \"met-resample\":\n",
    "                    for variable in self.met_variables:\n",
    "                        file = f\"{variable}.npz\"\n",
    "                        data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "                        if df.empty:\n",
    "                            df = pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"], columns=[variable])\n",
    "                        else:\n",
    "                            df = df.join(pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"], columns=[variable]))\n",
    "                else:\n",
    "                    file = listdir(data_folder)[0]\n",
    "                    data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "                    if df.empty:\n",
    "                        df = pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"], columns=[file.replace(\".npz\", \"\")])\n",
    "                    else:\n",
    "                        df = df.join(pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"], columns=[file.replace(\".npz\", \"\")]))\n",
    "\n",
    "            target_artifact = run.use_artifact(\"mortality-scaled:latest\")\n",
    "            target_folder = target_artifact.download()\n",
    "            data = np.load(path.join(target_folder, \"deaths.npz\"), allow_pickle=True)\n",
    "            df = df.join(pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"]*100000, columns=[\"deaths\"]))\n",
    "            df = df.dropna(axis=0)\n",
    "\n",
    "            # make new train, validation and test artifacts for regional scale data\n",
    "            if self.val_size:\n",
    "                index = {\"train\": df.index[:int(len(df.index)*self.train_size)],\n",
    "                         \"validat\": df.index[int(len(df.index)*self.train_size):int(len(df.index)*(self.train_size+self.val_size))],\n",
    "                        \"test\": df.index[int(len(df.index)*(self.train_size+self.val_size)):]}\n",
    "                subsets = [\"train\", \"validat\", \"test\"]\n",
    "            else:\n",
    "                index = {\"train\": df.index[:int(len(df.index)*self.train_size)],\n",
    "                        \"test\": df.index[int(len(df.index)*self.train_size):]}\n",
    "                subsets = [\"train\", \"test\"]\n",
    "            \n",
    "            scaler = MinMaxScaler()\n",
    "            x_scaler = scaler.fit(df.loc[index[\"train\"]].drop(\"deaths\", axis=1)) # Fit the scaler only to the training set distribution\n",
    "            for subset in subsets:\n",
    "                x = x_scaler.transform(df.loc[index[subset]].drop(\"deaths\", axis=1))\n",
    "                y = df.loc[index[subset]][\"deaths\"].values\n",
    "                z = df.loc[index[subset]].index\n",
    "                subset_data = wandb.Artifact(\n",
    "                            f\"xy_{subset}\", type=\"dataset\",\n",
    "                            description=f\"Input features (normalised) and targets for {subset}ing set.\",\n",
    "                            metadata={\"input_shape\":x.shape,\n",
    "                                     \"target_shape\":y.shape,\n",
    "                                     \"species\": self.species,\n",
    "                                      \"spatial_resolution\": self.spatial_resolution,\n",
    "                                      \"temporal_resolution\": self.temporal_resolution,\n",
    "                                      \"input_artifacts\": self.input_artifacts,\n",
    "                                      \"met_variables\": self.met_variables})\n",
    "                with subset_data.new_file(subset + \".npz\", mode=\"wb\") as file:\n",
    "                    np.savez(file, x=x, y=y, z=z)\n",
    "                run.log_artifact(subset_data)\n",
    "                \n",
    "    def read_data(self, artifact):\n",
    "        with wandb.init(project=\"AQmortality\", job_type=\"read-data\") as run:\n",
    "            data_artifact = run.use_artifact(f\"{artifact}:latest\")\n",
    "            data_folder = data_artifact.download()\n",
    "            file = artifact.replace(\"xy_\", \"\") + \".npz\"\n",
    "            data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "        return data[\"x\"], data[\"y\"]\n",
    "    \n",
    "    def train_and_log(self):\n",
    "        model_type = self.architecture.replace(\"_\", \"-\")\n",
    "        data_dict = {}\n",
    "        with wandb.init(project=\"AQmortality\", job_type=\"train-regional-model\", config=self.config) as run:\n",
    "            if self.val_size:\n",
    "                subsets = [\"train\", \"validat\"]\n",
    "            else:\n",
    "                subsets = [\"train\"]\n",
    "            for subset in subsets:\n",
    "                data_artifact = run.use_artifact(f\"xy_{subset}:latest\")\n",
    "                data_folder = data_artifact.download()\n",
    "                file = f\"{subset}.npz\"\n",
    "                data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "                data_dict.update({\"x_\"+subset: data[\"x\"], \"y_\"+subset: data[\"y\"]})\n",
    "            if model_type == \"linear-regressor\":\n",
    "                regressor = LinearRegression().fit(data_dict[\"x_train\"], data_dict[\"y_train\"])\n",
    "                data_dict.update({\"y_predict\": regressor.predict(data_dict[\"x_train\"])})\n",
    "                wandb.log({\"r_squared\": r2_score(data_dict[\"y_train\"], data_dict[\"y_predict\"]),\n",
    "                       \"mean_squared_error\": mean_squared_error(data_dict[\"y_train\"], data_dict[\"y_predict\"]),\n",
    "                       \"mean_absolute_percentage_error\": mape_score(data_dict[\"y_train\"], data_dict[\"y_predict\"])\n",
    "                      })\n",
    "            elif model_type == \"MLP-regressor\":\n",
    "                regressor, epoch = MLPRegression(self.hidden_layer_sizes).fit(data_dict[\"x_train\"], data_dict[\"y_train\"], \n",
    "                                                                              data_dict[\"x_validat\"], data_dict[\"y_validat\"], \n",
    "                                                                              self.batch_size, self.num_epochs, self.learning_rate)\n",
    "        # log trained model artifact â€“ include input features description\n",
    "            model = wandb.Artifact(\n",
    "                            f\"{model_type}\", type=\"model\",\n",
    "                            description=f\"{model_type} model.\",\n",
    "                            metadata={\"input_shape\":data_dict[\"x_train\"].shape,\n",
    "                                      \"target_shape\":data_dict[\"y_train\"].shape,\n",
    "                                      \"layer_sizes\": self.hidden_layer_sizes,\n",
    "                                      \"species\": self.species,\n",
    "                                      \"spatial_resolution\": self.spatial_resolution,\n",
    "                                      \"temporal_resolution\": self.temporal_resolution,\n",
    "                                      \"input_artifacts\": self.input_artifacts,\n",
    "                                      \"met_variables\": self.met_variables\n",
    "                                      })\n",
    "            if model_type == \"linear-regressor\":\n",
    "                with model.new_file(\"model.sav\", mode=\"wb\") as file:\n",
    "                    joblib.dump(regressor, file)\n",
    "            elif model_type == \"MLP-regressor\":\n",
    "                with model.new_file(\"model.tar\", mode=\"wb\") as file:\n",
    "                    torch.save({\"state_dict\": regressor.state_dict(),\n",
    "                    \"epoch\": epoch}, file)\n",
    "            run.log_artifact(model)\n",
    "    \n",
    "    \n",
    "    def test_and_log(self):\n",
    "        model_type = self.architecture.replace(\"_\", \"-\")\n",
    "        with wandb.init(project=\"AQmortality\", job_type=\"test-regional-model\", config=self.config) as run:\n",
    "            data_dict = {}\n",
    "            if self.val_size:\n",
    "                subsets = [\"train\", \"validat\", \"test\"]\n",
    "            else:\n",
    "                subsets = [\"train\", \"test\"]\n",
    "            for subset in subsets:\n",
    "                data_artifact = run.use_artifact(f\"xy_{subset}:latest\")\n",
    "                data_folder = data_artifact.download()\n",
    "                file = f\"{subset}.npz\"\n",
    "                data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "                data_dict.update({\"x_\"+subset: data[\"x\"], \"y_\"+subset: data[\"y\"], subset+\"_dates\": data[\"z\"]})\n",
    "            # use trained model artifact\n",
    "            model_artifact = run.use_artifact(f\"{model_type}:latest\")\n",
    "            model_folder = model_artifact.download()\n",
    "            if model_type == \"linear-regressor\":\n",
    "                regressor = joblib.load(path.join(model_folder, \"model.sav\"))\n",
    "                for subset in subsets:\n",
    "                    data_dict.update({f\"y_{subset}_predict\": regressor.predict(data_dict[f\"x_{subset}\"])})\n",
    "    \n",
    "                wandb.log({\"r_squared\": r2_score(data_dict[\"y_test\"], data_dict[\"y_test_predict\"]), \n",
    "                           \"mean_squared_error\": mean_squared_error(data_dict[\"y_test\"], data_dict[\"y_test_predict\"]), \n",
    "                           \"mean_absolute_percentage_error\": mape_score(data_dict[\"y_test\"], data_dict[\"y_test_predict\"])\n",
    "                          })\n",
    "            elif model_type == \"MLP-regressor\":\n",
    "                regressor = MLPRegression(self.hidden_layer_sizes)\n",
    "                checkpoint = torch.load(path.join(model_folder, \"model.tar\"))\n",
    "                regressor.evaluate(checkpoint, data_dict[\"x_test\"], data_dict[\"y_test\"], self.batch_size)\n",
    "                for subset in subsets:\n",
    "                    data_dict.update({f\"y_{subset}_predict\": regressor.predict(checkpoint, data_dict[f\"x_{subset}\"], data_dict[f\"y_{subset}\"], self.batch_size)})\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HealthModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.preprocess_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = model.test_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "if config[\"val_size\"]:\n",
    "    subsets = [\"train\", \"validat\", \"test\"]\n",
    "else:\n",
    "    subsets = [\"train\", \"test\"]\n",
    "for subset in subsets:\n",
    "    subset_df = pd.DataFrame(index=pd.DatetimeIndex(data_dict[f\"{subset}_dates\"]), data={\"observed\":data_dict[f\"y_{subset}\"], \"predicted\":data_dict[f\"y_{subset}_predict\"]})\n",
    "    if df.empty:\n",
    "        df = subset_df.copy()\n",
    "    else:\n",
    "        df = df.append(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n",
    "plt.axvline(data_dict[\"train_dates\"].max(), color=\"grey\")\n",
    "if config[\"val_size\"]:\n",
    "    plt.axvline(data_dict[\"validat_dates\"].max(), color=\"grey\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"deaths per 100,000\")\n",
    "model_title = config[\"architecture\"].replace(\"_\", \" \")\n",
    "plt.suptitle(f\"London mortality predictions by {model_title}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AQmort",
   "language": "python",
   "name": "aqmort"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
