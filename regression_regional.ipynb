{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_classes import LAQNData, HealthData, MetData, IncomeData\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports for classes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from os import path, listdir, environ\n",
    "import wandb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"architecture\": \"MLP_regressor\",\n",
    "    \"train_size\": 0.7,\n",
    "    \"val_size\": 0.15, # Set to False if using linear regressor\n",
    "    \"species\": \"NO2\",\n",
    "    \"spatial_resolution\": \"regional\",\n",
    "    \"temporal_resolution\": \"daily\",\n",
    "    \"input_artifacts\": [\"laqn-regional\", \"met-resample\", \"income-regional\"],\n",
    "    \"met_variables\": [\"temperature\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be moved to model_classes script when finished developing.\n",
    "\n",
    "class MLPRegression():\n",
    "    def __init__(self, hidden_layer_sizes, output_layer_size=1):\n",
    "        self.hl_sizes = hidden_layer_sizes\n",
    "        self.out_sizes = output_layer_size\n",
    "        # Initialise the MLPArchitecture class\n",
    "        self.model = MLPArchitecture(self.hl_sizes, self.out_sizes)\n",
    "        # Set random seed for torch\n",
    "        self.random_seed = 1\n",
    "        \n",
    "    def fit(self, x_train, y_train, x_val, y_val, batch_size, num_epochs, learning_rate):\n",
    "        # Training code that loops through epochs\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        model = self.model\n",
    "        \n",
    "        training_dataset = MLPDataset(x_train, y_train, noise_std=config.noise_standard_deviation)\n",
    "        validation_dataset = MLPDataset(x_val, y_val)\n",
    "\n",
    "        training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "        validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimiser = Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "        \n",
    "        training_loss_history = []\n",
    "        validation_loss_history = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training set\n",
    "            model.train()\n",
    "            loss_sum = 0  # for storing\n",
    "\n",
    "            for batch_num, data in enumerate(training_dataloader):\n",
    "                inputs_training = data[\"inputs\"]\n",
    "                targets_training = data[\"targets\"]\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "                # Run the forward pass\n",
    "                y_predict = model(inputs_training)\n",
    "                # Compute the loss and gradients\n",
    "                single_loss = criterion(y_predict, targets_training)\n",
    "                single_loss.backward()\n",
    "                # Update the parameters\n",
    "                optimiser.step()\n",
    "\n",
    "                # Calculate loss for storing\n",
    "                loss_sum += single_loss.item()*data[\"targets\"].shape[0]  # Account for different batch size with final batch\n",
    "\n",
    "            training_loss_history.append(loss_sum / len(training_dataset))  # Save the training loss after every epoch\n",
    "\n",
    "            # Do the same for the validation set\n",
    "            model.eval()\n",
    "            validation_loss_sum = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_num, data in enumerate(validation_dataloader):\n",
    "                    inputs_val = data[\"inputs\"]\n",
    "                    targets_val = data[\"targets\"]\n",
    "                    y_predict_validation = model(inputs_val)\n",
    "                    single_loss = criterion(y_predict_validation, targets_val)\n",
    "                    validation_loss_sum += single_loss.item()*data[\"targets\"].shape[0]\n",
    "                \n",
    "            # Store the model with smallest validation loss. Check if the validation loss is the lowest BEFORE\n",
    "            # saving it to loss history (otherwise it will not be lower than itself)\n",
    "            if (not validation_loss_history) or validation_loss_sum / len(validation_dataset) < min(validation_loss_history):\n",
    "                best_model = deepcopy(model)\n",
    "                best_epoch = epoch\n",
    "            validation_loss_history.append(validation_loss_sum / len(validation_dataset))  # Save the val loss every epoch.\n",
    "\n",
    "\n",
    "            wandb.log({\"epoch\": epoch, \n",
    "                       \"training_loss\": loss_sum / len(training_dataset),\n",
    "                      \"validation_loss\": validation_loss_sum / len(validation_dataset)}, \n",
    "                      step=epoch)\n",
    "            \n",
    "#                 # Save the model every 2 epochs\n",
    "#             if epoch % 2 == 0:\n",
    "#                 torch.save({\n",
    "#                     \"total_epochs\": epoch,\n",
    "#                     \"final_state_dict\": model.state_dict(),\n",
    "#                     \"optimiser_state_dict\": optimiser.state_dict(),\n",
    "#                     \"training_loss_history\": training_loss_history,\n",
    "#                     \"best_state_dict\": best_model.state_dict(),\n",
    "#                     \"best_epoch\": best_epoch,\n",
    "#                     \"validation_loss_history\": validation_loss_history,\n",
    "#                 }, save_path)\n",
    "\n",
    "\n",
    "        return  best_model, best_epoch\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Produce outputs y\n",
    "        return y\n",
    "    \n",
    "class MLPArchitecture(nn.Module):\n",
    "    def __init__(self, hl_sizes, out_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hl_sizes)-1):\n",
    "            self.hidden_layers.append(nn.Linear(hl_sizes[i], hl_sizes[i + 1], bias=False))\n",
    "        self.output_layer = nn.Linear(hl_sizes[-1], out_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through each layer\n",
    "        x = x.view(x.shape[0], x.shape[1] * x.shape[2])\n",
    "        # Feedforward\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = torch.relu(hidden_layer(x))\n",
    "        return self.output_layer(x)\n",
    "       \n",
    "class MLPDataset(Dataset):\n",
    "    def __init__(self, x, y, noise_std=False):\n",
    "        self.x = torch.from_numpy(x).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        self.noise_std = noise_std  # Standard deviation of Gaussian noise\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.size()[0]\n",
    "\n",
    "    def nfeatures(self):\n",
    "        return self.x.size()[-1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x1 = self.x[index]\n",
    "        y1 = self.y[index]\n",
    "        if self.noise_std:\n",
    "            noise = torch.randn_like(x1)*self.noise_std\n",
    "            x1 = x1 + noise\n",
    "        return {\"inputs\": x1, \"targets\": y1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be moved to model_classes script when finished developing.\n",
    "\n",
    "class HealthModel():\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.architecture = config[\"architecture\"]\n",
    "        self.train_size = config[\"train_size\"]\n",
    "        self.val_size = config[\"val_size\"]\n",
    "        self.species = config[\"species\"]\n",
    "        self.spatial_resolution = config[\"spatial_resolution\"]\n",
    "        self.temporal_resolution = config[\"temporal_resolution\"]\n",
    "        self.input_artifacts = config[\"input_artifacts\"]\n",
    "        self.met_variables = config[\"met_variables\"]\n",
    "\n",
    "    def preprocess_and_log(self):\n",
    "        with wandb.init(project=\"AQmortality\", job_type=\"split-normalise-data\") as run:\n",
    "            df = pd.DataFrame()\n",
    "            # use dataset artifacts\n",
    "            for artifact in self.input_artifacts:\n",
    "                data_artifact = run.use_artifact(f\"{artifact}:latest\")\n",
    "                data_folder = data_artifact.download()\n",
    "                if artifact == \"met-resample\":\n",
    "                    for variable in self.met_variables:\n",
    "                        file = f\"{variable}.npz\"\n",
    "                        data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "                        if df.empty:\n",
    "                            df = pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"], columns=[variable])\n",
    "                        else:\n",
    "                            df = df.join(pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"], columns=[variable]))\n",
    "                else:\n",
    "                    file = listdir(data_folder)[0]\n",
    "                    data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "                    if df.empty:\n",
    "                        df = pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"], columns=[file.replace(\".npz\", \"\")])\n",
    "                    else:\n",
    "                        df = df.join(pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"], columns=[file.replace(\".npz\", \"\")]))\n",
    "\n",
    "            target_artifact = run.use_artifact(\"mortality-scaled:latest\")\n",
    "            target_folder = target_artifact.download()\n",
    "            data = np.load(path.join(target_folder, \"deaths.npz\"), allow_pickle=True)\n",
    "            df = df.join(pd.DataFrame(index=pd.DatetimeIndex(data[\"x\"]), data=data[\"y\"]*100000, columns=[\"deaths\"]))\n",
    "            df = df.dropna(axis=0)\n",
    "\n",
    "            # make new train, validation and test artifacts for regional scale data\n",
    "            if self.val_size:\n",
    "                index = {\"train\": df.index[:int(len(df.index)*self.train_size)],\n",
    "                         \"validat\": df.index[int(len(df.index)*self.train_size):int(len(df.index)*(self.train_size+self.val_size))],\n",
    "                        \"test\": df.index[int(len(df.index)*(self.train_size+self.val_size)):]}\n",
    "                subsets = [\"train\", \"validat\", \"test\"]\n",
    "            else:\n",
    "                index = {\"train\": df.index[:int(len(df.index)*self.train_size)],\n",
    "                        \"test\": df.index[int(len(df.index)*self.train_size):]}\n",
    "                subsets = [\"train\", \"test\"]\n",
    "            \n",
    "            scaler = MinMaxScaler()\n",
    "            x_scaler = scaler.fit(df.loc[index[\"train\"]].drop(\"deaths\", axis=1)) # Fit the scaler only to the training set distribution\n",
    "            for subset in subsets:\n",
    "                x = x_scaler.transform(df.loc[index[subset]].drop(\"deaths\", axis=1))\n",
    "                y = df.loc[index[subset]][\"deaths\"].values\n",
    "                z = df.loc[index[subset]].index\n",
    "                subset_data = wandb.Artifact(\n",
    "                            f\"xy_{subset}\", type=\"dataset\",\n",
    "                            description=f\"Input features (normalised) and targets for {subset}ing set.\",\n",
    "                            metadata={\"input_shape\":x.shape,\n",
    "                                     \"target_shape\":y.shape,\n",
    "                                     \"species\": self.species,\n",
    "                                      \"spatial_resolution\": self.spatial_resolution,\n",
    "                                      \"temporal_resolution\": self.temporal_resolution,\n",
    "                                      \"input_artifacts\": self.input_artifacts,\n",
    "                                      \"met_variables\": self.met_variables})\n",
    "                with subset_data.new_file(subset + \".npz\", mode=\"wb\") as file:\n",
    "                    np.savez(file, x=x, y=y, z=z)\n",
    "                run.log_artifact(subset_data)\n",
    "                \n",
    "    def read_data(self, artifact):\n",
    "        with wandb.init(project=\"AQmortality\", job_type=\"read-data\") as run:\n",
    "            data_artifact = run.use_artifact(f\"{artifact}:latest\")\n",
    "            data_folder = data_artifact.download()\n",
    "            file = artifact.replace(\"xy_\", \"\") + \".npz\"\n",
    "            data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "        return data[\"x\"], data[\"y\"]\n",
    "    \n",
    "    def train_and_log(self):\n",
    "        model_type = self.architecture.replace(\"_\", \"-\")\n",
    "        with wandb.init(project=\"AQmortality\", job_type=\"train-regional-model\", config=self.config) as run:\n",
    "            artifact = \"xy_train\"\n",
    "            data_artifact = run.use_artifact(f\"{artifact}:latest\")\n",
    "            data_folder = data_artifact.download()\n",
    "            file = artifact.replace(\"xy_\", \"\") + \".npz\"\n",
    "            data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "            x_train, y_train = data[\"x\"], data[\"y\"]\n",
    "            if model_type == \"linear-regressor\":\n",
    "                regressor = LinearRegression().fit(x_train, y_train)\n",
    "            elif model_type == \"MLP-regressor\":\n",
    "                regressor, epoch = MLPRegression().fit(x_train, y_train)\n",
    "            # log model training metrics\n",
    "            wandb.log({\"r_squared\": r2_score(y_train, regressor.predict(x_train)),\n",
    "                       \"mean_squared_error\": mean_squared_error(y_train, regressor.predict(x_train)),\n",
    "                       \"mean_absolute_percentage_error\": self.mape_score(y_train, regressor.predict(x_train))\n",
    "                      })\n",
    "\n",
    "        # log trained model artifact – include input features description\n",
    "            model = wandb.Artifact(\n",
    "                            f\"{model_type}\", type=\"model\",\n",
    "                            description=f\"{model_type} model.\",\n",
    "                            metadata={\"input_shape\":x_train.shape,\n",
    "                                      \"target_shape\":y_train.shape,\n",
    "                                      \"species\": self.species,\n",
    "                                      \"spatial_resolution\": self.spatial_resolution,\n",
    "                                      \"temporal_resolution\": self.temporal_resolution,\n",
    "                                      \"input_artifacts\": self.input_artifacts,\n",
    "                                      \"met_variables\": self.met_variables\n",
    "                                      })\n",
    "            if model_type == \"linear-regressor\":\n",
    "                with model.new_file(\"model.sav\", mode=\"wb\") as file:\n",
    "                    joblib.dump(regressor, file)\n",
    "            elif model_type == \"MLP-regressor\":\n",
    "                with model.new_file(\"model.tar\", mode=\"wb\") as file:\n",
    "                    torch.save({\"model\": regressor.state_dict(),\n",
    "                    \"epoch\": epoch}, file)\n",
    "            run.log_artifact(model)\n",
    "    \n",
    "    \n",
    "    def test_and_log(self):\n",
    "        model_type = self.architecture.replace(\"_\", \"-\")\n",
    "        with wandb.init(project=\"AQmortality\", job_type=\"test-regional-model\", config=self.config) as run:\n",
    "            data_dict = {}\n",
    "            if self.val_size:\n",
    "                subsets = [\"train\", \"validat\", \"test\"]\n",
    "            else:\n",
    "                subsets = [\"train\", \"test\"]\n",
    "            for subset in subsets:\n",
    "                data_artifact = run.use_artifact(f\"xy_{subset}:latest\")\n",
    "                data_folder = data_artifact.download()\n",
    "                file = f\"{subset}.npz\"\n",
    "                data = np.load(path.join(data_folder, file), allow_pickle=True)\n",
    "                data_dict.update({\"x_\"+subset: data[\"x\"], \"y_\"+subset: data[\"y\"], subset+\"_dates\": data[\"z\"]})\n",
    "            # use trained model artifact\n",
    "            model_artifact = run.use_artifact(f\"{model_type}:latest\")\n",
    "            model_folder = model_artifact.download()\n",
    "            regressor = joblib.load(path.join(model_folder, \"model.sav\"))\n",
    "    \n",
    "        # test linear regression model\n",
    "        # log model test metrics\n",
    "            wandb.log({\"r_squared\": r2_score(data_dict[\"y_test\"], regressor.predict(data_dict[\"x_test\"])),\n",
    "                       \"mean_squared_error\": mean_squared_error(data_dict[\"y_test\"], regressor.predict(data_dict[\"x_test\"])),\n",
    "                       \"mean_absolute_percentage_error\": self.mape_score(data_dict[\"y_test\"], regressor.predict(data_dict[\"x_test\"]))\n",
    "                      })\n",
    "        return regressor, data_dict\n",
    "    \n",
    "    def mape_score(self, targets, predictions):\n",
    "        zero_indices = np.where(targets == 0)\n",
    "        targets_drop_zero = np.delete(targets, zero_indices)\n",
    "        prediction_drop_zero = np.delete(predictions, zero_indices)\n",
    "        mape = np.sum(np.abs(targets_drop_zero - prediction_drop_zero)/targets_drop_zero) * 100/len(targets_drop_zero)\n",
    "        return mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HealthModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichellewl\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.30<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">deft-donkey-166</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/michellewl/AQmortality\" target=\"_blank\">https://wandb.ai/michellewl/AQmortality</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/michellewl/AQmortality/runs/3el6qsh1\" target=\"_blank\">https://wandb.ai/michellewl/AQmortality/runs/3el6qsh1</a><br/>\n",
       "                Run data is saved locally in <code>/home/users/mwlw3/AQmortality/wandb/run-20210524_155113-3el6qsh1</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 173<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d57dbb33eee44249d8098ecd339a1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.25MB of 0.25MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/users/mwlw3/AQmortality/wandb/run-20210524_155113-3el6qsh1/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/users/mwlw3/AQmortality/wandb/run-20210524_155113-3el6qsh1/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">deft-donkey-166</strong>: <a href=\"https://wandb.ai/michellewl/AQmortality/runs/3el6qsh1\" target=\"_blank\">https://wandb.ai/michellewl/AQmortality/runs/3el6qsh1</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.preprocess_and_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_and_log(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor, data_dict = model.test_and_log(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "if config[\"val_size\"]:\n",
    "    subsets = [\"train\", \"validat\", \"test\"]\n",
    "else:\n",
    "    subsets = [\"train\", \"test\"]\n",
    "for subset in subsets:\n",
    "    subset_df = pd.DataFrame(index=pd.DatetimeIndex(data_dict[f\"{subset}_dates\"]), data={\"observed\":data_dict[f\"y_{subset}\"], \"predicted\":regressor.predict(data_dict[f\"x_{subset}\"])})\n",
    "    if df.empty:\n",
    "        df = subset_df.copy()\n",
    "    else:\n",
    "        df = df.append(subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n",
    "plt.axvline(data_dict[\"train_dates\"].max(), color=\"grey\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"deaths per 100,000\")\n",
    "model_title = config[\"architecture\"].replace(\"_\", \" \")\n",
    "plt.suptitle(\"London mortality predictions by linear regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AQmort",
   "language": "python",
   "name": "aqmort"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
